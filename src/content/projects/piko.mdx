---
title: "Piko"
description: "A soft robotic prototype exploring whether peripheral physical feedback can change phone usage behavior. Detects doom-scrolling posture with 91% accuracy using a custom-trained computer vision model, responding with tactile feedback and emotion-based gestures."
category: "embodied"
order: 5
heroImage: "/assets/projects/piko/cover.png"
bentoSize: "large"
tags: ["Physical AI", "Computer Vision", "Embodied Interaction"]
meta:
  Course: "Building User-Centered Sensing Systems"
  Duration: "4 Weeks"
  Team: "Ashveen Banga, Chris Wu, Leo Liu, Narayan"
  Contribution: "Interaction Design, Computer Vision"
  Advisor: "Mayank Goel, Yuvraj Agarwal"
  Year: "2025"
sections:
  - id: "problem-opportunity"
    label: "Why build it?"
  - id: "approach"
    label: "What's the approach?"
  - id: "implementation"
    label: "How does it work?"
  - id: "interaction-model"
    label: "How is it built?"
  - id: "outcome"
    label: "What's the outcome?"
---

import ProjectSection from '../../components/ProjectSection.astro';
import FullWidthImage from '../../components/FullWidthImage.astro';
import VideoEmbed from '../../components/VideoEmbed.astro';
import PlaceholderImage from '../../components/PlaceholderImage.astro';

<FullWidthImage src="/assets/projects/piko/bts.png" height="500px" caption="Piko: The soft robotic companion" constrain={false} />

<ProjectSection title="Piko" id="piko-intro" hideTitle={true}>
    <div slot="intro">
        <p>
            In an era of hyper-digital fatigue, Piko explores how "softness" can serve as a medium for emotional regulation. It is a handheld robotic companion that mimics physiological rhythms, responding to the user's touch and presence with life-like breathing patterns.
        </p>
    </div>
</ProjectSection>

<ProjectSection title="The Infinite Scroll Trap" id="problem-opportunity">
    <p>
        Modern digital interfaces are engineered for addiction. Infinite scroll algorithms leverage variable reward schedules to keep users in a state of passive consumption, often leading to "Doom Scrolling"â€”a compulsive behavior that negatively impacts mental well-being and time management.
    </p>
    <p>Modern interactive rituals often demand high cognitive attentionâ€”constant notifications, infinite feeds, and bright screens. The opportunity was to design an object that exists in the "periphery" of our attention, providing comfort without demanding interaction.</p>
    
    <FullWidthImage src="/assets/projects/piko/doom scrooling.png" height="400px" caption="Addressing the stress of digital depletion" />
</ProjectSection>

<ProjectSection title="Social Robotics as a Nudge" id="approach">
    <p>Drawing from "Social Tech" frameworks and soft-robotic research, the approach focused on non-verbal communication. By studying how humans comfort each other through physical presence, I developed a system that uses breathing as a primary feedback loop.</p>
    
    <FullWidthImage src="/assets/projects/piko/social-tech.png" height="300px" caption="Social Technology Framework" />
    
    <hr class="feature-divider" />

    <p style="font-weight: 700;">Engineering the Vision System</p>
    <p>Developing a robust detection system required moving beyond off-the-shelf solutions to ensure both accuracy and privacy. We engineered a custom small machine learning model specifically trained to identify the "Doom Scroll" postureâ€”a unique combination of hand positioning and phone orientation. By curating a  dataset of <a href="https://universe.roboflow.com/cvsensing/pico-sensing" target="_blank" style="text-decoration: underline; text-underline-offset: 4px; color: var(--accent-secondary);">over 400 images using Roboflow </a>, we fine-tuned the computer vision algorithm to distinguish between active phone use and passive scrolling. This targeted training approach allows the system to intervene only when necessary, minimizing false positives and ensuring the robot's reactions feel intuitive rather than intrusive.</p>
    
    <FullWidthImage src="/assets/projects/piko/cv.png" height="300px" caption="Training a custom computer vision model" />
</ProjectSection>

<ProjectSection title="Piko's Anatomy" id="implementation">
    <p>To bring Piko to life, we needed a hardware architecture that could support real-time vision, complex motion, and audio processing within a compact, huggable form factor. The internal chassis is a custom-designed 3D printed skeleton that houses the distributed computing system.</p>
    
    <FullWidthImage src="/assets/projects/piko/Piko inside-out.webp" height="500px" caption="Internal mechanics vs External form" />

    <div style="display: flex; flex-direction: column; gap: 1rem; margin-top: 1rem;">
        <div>
            <span class="attribute-pill" style="background: #D6820B; color: #ffffff; margin-bottom: 0.75rem; font-weight: 700;">The Vision System</span>
            <p>At the core of Piko's awareness is a Raspberry Pi Camera Module 3. This wide-angle sensor provides the raw visual data needed for the "Inside-Out" detection model, allowing the robot to see the user's hands and phone without capturing facial data.</p>
        </div>

        <hr class="feature-divider" style="margin: 0;" />

        <div>
            <span class="attribute-pill" style="background: #D6820B; color: #ffffff; margin-bottom: 0.75rem; font-weight: 700;">The Brain & Voice</span>
            <p>Processing power is split to handle the load. A Raspberry Pi Zero 2 W acts as the central nervous system, managing the logic and communication. It is paired with a WM8960 Audio HAT, which drives the onboard speakers and captures ambient sound, enabling Piko to "speak" and react audibly to the user's behavior.</p>
        </div>

        <hr class="feature-divider" style="margin: 0;" />

        <div>
            <span class="attribute-pill" style="background: #D6820B; color: #ffffff; margin-bottom: 0.75rem; font-weight: 700;">The Motor Control</span>
            <p>Two SG90 Micro Servo Motors control the arm and head movements, allowing for gestures ranging from a gentle "wave" to an emphatic "clap." These motors are orchestrated to create fluid, organic motion that mimics biological responses rather than robotic jerks.</p>
        </div>
    </div>

    <FullWidthImage src="/assets/projects/piko/xray.png" height="500px" caption="Deep dive: Internal X-ray" />
    
    <hr class="feature-divider" />

    <p>Building a robot that "watches" you requires rigorous ethical boundaries. We addressed this by embedding privacy directly into the physical form. The camera is strictly fixed downward, focusing solely on the lap area to detect phone usage without capturing faces.</p>

    <div style="display: flex; flex-direction: column; gap: 1rem; margin-top: 1rem;">

        <hr class="feature-divider" style="margin: 0;" />

        <div>
            <span class="attribute-pill" style="background: #b8ff97ff; color: #282828; margin-bottom: 0.75rem; font-weight: 700;">Physical Gating</span>
            <p>To prevent intrusive monitoring, verbal interaction is physically gated. The robot remains silent until a manual "Tap-to-Speak" button is pressed, ensuring the user explicitly invites audio feedback. Furthermore, a "Double-Tap" instantly engages "Quiet Mode," providing a hardware-level kill switch that empowers the user to silence the companion during deep work sessions.</p>
        </div>

        <hr class="feature-divider" style="margin: 0;" />

        <div>
            <span class="attribute-pill" style="background: #ff7878ff; color: #ffffff; margin-bottom: 0.75rem; font-weight: 700;">Current Limitations</span>
            <p>However, the prototype reveals ethical gaps. To achieve real-time responsiveness, it currently relies on cloud processing rather than on-device computation, and lacks a hardware indicator light for the camera. A production-ready iteration would need to shift all Computer Vision processing to the local device (Edge AI) to ensure that no visual data ever leaves the robot's chassis.</p>
        </div>
    </div>

    <FullWidthImage src="/assets/projects/piko/privacy.png" height="300px" caption="Haptic privacy explorations" />
    
    <hr class="feature-divider" />

    <div>
        <span class="attribute-pill" style="background: #E2DFD2; color: #282828; margin-bottom: 0.75rem; font-weight: 700;">Interaction Design: Emotive Actuation</span>
        <p>We developed a small library of physical gestures to translate the system's logic into social cues. Using the servo array, Piko moves through four distinct emotional states: Curiosity, Joy, Shyness, and Anger. These non-verbal signals allow the device to critique digital habits without being intrusive, leveraging our natural empathetic response to biological motion to break the user's focus loop.</p>
    </div>

    <FullWidthImage src="/assets/projects/piko/interactions.gif" height="300px" caption="Interaction library" />
</ProjectSection>

<ProjectSection title="Technical Architecture" id="interaction-model">
    <p>The architecture bridges hardware sensing with behavioral responses. The robot doesn't just "breathe"; it reacts to how the user holds it, adjusting its pace to match or soothe the user's own rhythm.</p>
    
    <p>To achieve low-latency interactions, we distributed the workload across a three-node ecosystem. This split architecture balances heavy processing with real-time responsiveness.</p>

    <div style="display: flex; flex-direction: column; gap: 1rem; margin-top: 1rem;">

        <hr class="feature-divider" style="margin: 0;" />

        <div>
            <span class="attribute-pill" style="background: #E2DFD2; color: #282828; margin-bottom: 0.75rem; font-weight: 700;">ðŸ‘€ â€” Laptop + MQTT</span>
            <p>Handles the computational heavy lifting. It runs the Computer Vision model to detect "Hand + Phone" triggers and broadcasts events instantly via MQTT.</p>
        </div>

        <hr class="feature-divider" style="margin: 0;" />

        <div>
            <span class="attribute-pill" style="background: #E2DFD2; color: #282828; margin-bottom: 0.75rem; font-weight: 700;">ðŸ§  â€” Raspberry Pi A</span>
            <p>Manages logic and personality. It listens for triggers and orchestrates the response, calling Google Gemini 2.0 Flash for audio and Pushover for phone notifications.</p>
        </div>

        <hr class="feature-divider" style="margin: 0;" />

        <div>
            <span class="attribute-pill" style="background: #E2DFD2; color: #282828; margin-bottom: 0.75rem; font-weight: 700;">ðŸ’ª â€” Raspberry Pi B</span>
            <p>Dedicated to motion control. It isolates servo noise and power draw, receiving commands to execute specific emotional gestures like "Angry" or "Curious."</p>
        </div>
    </div>
    <FullWidthImage src="/assets/projects/piko/architecture.gif" height="400px" caption="System communication flow" />
</ProjectSection>

<ProjectSection title="Project Demo" id="outcome">
    <p>A walkthrough of the fully functional prototype, demonstrating the computer vision detection triggers and the robot's emotive feedback loops in real-time.</p>
    
    <VideoEmbed type="vimeo" id="1163205367" title="Piko Interaction Flow" />
</ProjectSection>
